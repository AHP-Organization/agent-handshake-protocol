# AHP Whitepaper Critique

**Critiqued version:** AHP-WHITEPAPER-0.1.md (Draft 0.1, revised 3 — 2026-02-22)
**Critique date:** 2026-02-22 09:30 UTC
**Live test data:** `/tmp/ahp-report-latest.json` — present, run at 09:04 UTC
**Prior critiques:** 06:30, 07:30, 08:30 UTC — this is the fourth iteration

---

## Preface: What Has Been Fixed Across Four Revisions

Substantial progress across revisions. For the record:

- ✓ Naive-baseline straw-man labelled as upper-bound non-competitive reference
- ✓ RAG-baseline comparison implemented with API-measured token counts — **live data confirms real numbers** (289–951 tokens AHP site, 425–650 Nate site)
- ✓ Cache-busting nonces implemented — AHP token stddev is now real (±5–42 tokens)
- ✓ T12 false-pass fixed: uses known product IDs, price regex, failure-phrase exclusion — **live data confirms genuine quote delivery**
- ✓ T15/T16 ID swap fixed
- ✓ T17 (MODE3 auth), T18 (session turn limit), T19 (oversized body) added
- ✓ T08 false-pass *identified* and `no_memory_phrases` exclusion list added to v4 code
- ✓ T20 (rate-limit header verification) added to v4 code
- ✓ Latency profile *redesigned* in v4 code (10 cold nonces + 10 hot)
- ✓ T17 failure labelled "CRITICAL KNOWN DEFECT" with production deployment warning
- ✓ Session memory limitation explicitly disclosed in §3.1
- ✓ Related work, conformance scope, cost model, in-page notice limitations all addressed
- ✓ Appendix updated to reference correct repo URL

What follows focuses on what remains wrong — including three findings that are confirmed or newly introduced by the 09:04 live test run.

---

## Critical Problems (must fix)

### 1. The paper claims v4 suite results; the live data is provably from v3

This is the most important finding of this critique cycle. The whitepaper's Section 4.2 states: *"Results from 2026-02-22 09:00 UTC run — first complete run with cache-busted RAG baseline API-measured (v4 suite)."* The live data was generated at **09:04:52 UTC**. The v4 test runner code was last modified at **09:06:22 UTC** — two minutes *after* the test ran.

Three forensic markers in the live JSON confirm it was generated by the v3 suite, not v4:

**Marker 1 — T20 is absent.** The v4 suite added `test_ratelimit_headers` as T20. The live results contain 19 tests (T01–T19); T20 is not present. If v4 had run, there would be 20 tests.

**Marker 2 — T08 passes in the live data.** The T08 result shows `passed: true` with the note: *"Session memory verified — turn 2 references MODE1 from turn 1."* The actual turn 2 answer (also in the notes) reads: *"I DON'T HAVE ANY RECORD OF PREVIOUS QUESTIONS YOU'VE ASKED. THIS IS THE FIRST MESSAGE IN OUR CONVERS..."* The v4 `no_memory_phrases` list explicitly includes `"DON'T HAVE ANY RECORD"` and `"THIS IS THE FIRST MESSAGE"` — if v4 had run, T08 would have failed. It passed because v3's keyword-only logic fired on "FIRST."

**Marker 3 — Latency profile shows no v4 fields.** The v4 profile adds a `"design"` key and per-sample `"type"` field and computes `cold_mean_ms` / `hot_p50_ms`. The live profile has none of these — `design` is absent, `cold_mean_ms` is null, all 20 samples report no `type`. The v4 redesign (10 cold nonces + 10 hot) never executed.

**The consequence:** The paper presents the data tables as "v4 suite results" but they are v3 results. The author manually applied editorial knowledge to mark T08 as ✗ in the conformance table (correctly), but the underlying run is v3. This creates a credibility issue: a reader who checks the live JSON file against the paper's claims will find a test whose live result contradicts the paper's table, and a test count that doesn't match the claimed suite version. The paper must either re-run with the actual v4 suite and update the tables, or clearly label the data as "from v3 run; T08 result manually corrected based on known server behaviour."

### 2. Three inconsistent conformance numbers appear in the same paper

The paper contains three different conformance counts that are mutually inconsistent:

- **Abstract (line 14):** *"The reference deployment passes **16 of 19 tested** conformance checks (v4 suite)"*
- **Section 4.1 (line 182):** *"**16/20 tested (80%)** on reference deployment (v4 suite logic)"*
- **Conclusion (line 410):** *"We demonstrated **18/19 conformance** on the reference deployment"*

All three are arithmetically or logically wrong simultaneously:

The live run (v3) shows **18/19 passed, 1 failed (T17 only)**. With T08 correctly counted as a failure (as the paper intends), it becomes 17/19 — not 16. The abstract's "16 of 19" implies three failures but only two are named. The Section 4.1 "16/20" uses a denominator of 20 (including T20-pending) and counts correctly (20 − T08 − T17 − T20-pending = 17... still not 16). The Conclusion's "18/19" is the stale v3 figure before T08 was corrected.

The correct statement, for a v4 run with T08 properly failing: **17/20 tested (T08 ✗ no session memory, T17 ✗ known auth defect, T20 pending).** Every other framing in the paper is wrong. A reviewer comparing abstract to conclusion to table will immediately flag this.

### 3. T08 is still a live false pass — the fix exists in code but has not run

The v4 `test_multiturn` code is correctly written. But it has never executed against the server. The live T08 result in the backing data remains `passed: true` with the server explicitly saying it has no session memory. The conformance table marks T08 as ✗ based on editorial knowledge, not test evidence.

This matters beyond bookkeeping. The reference server's session_id mechanism issues a UUID, enforces a 10-turn limit (T18 passes), but passes zero history to the LLM on subsequent turns. This is confirmed by the server's literal response: *"I don't have any record of previous questions you've asked. This is the first message in our conversation."*

A practitioner reading §3.1 ("session management: in-memory sessions with 10-minute TTL and 10-turn limit") and §5.4 ("visiting agent side underspecified") might not realise that the reference implementation they are cloning has no multi-turn context. The spec requires session memory (§6.2). The reference implementation doesn't provide it. This gap is disclosed in §3.1 — that disclosure is good — but it needs to be prominent in the implementation notes and README, not just a paragraph in the evaluation.

### 4. The latency profile is still all cache hits — v4 cold-cache redesign never ran

Despite the v4 profile redesign (10 cold nonces + 10 hot), the live data shows **all 20 samples are cache hits** (`cached_n: 20, uncached_n: 0, cold_mean_ms: null`). The redesign was committed after the test ran.

The paper's Section 4.3 latency table — which the author intends to show *both* cold and cached cohorts — is still reporting only cached performance. The p50 of 20.6ms, p95 of 226.4ms, and min of 8.5ms are entirely from the cache layer. The cold-cache performance (2,500–5,200ms measured from individual test queries) remains inferred from single-shot measurements, not from a systematic profile run.

The latency comparison table in §4.3 ("Cold-cache latency inferred from individual tests") is the right framing but is not a substitute for a proper cold-cache latency profile. Once the v4 suite actually runs, the cold profile will measure 10 unique nonce queries — but these queries ("Explain the AHP protocol overview [latency-cold-XXXXXXXX]") are all variants of the same question, which may not represent the distribution of real visiting-agent queries. For a more credible profile, the cold samples should span several different capability types and question styles.

---

## Secondary Issues

### 5. RAG latency for "rate limits" query has an unexplained ±1,100ms stddev

The live data shows the RAG latency for "What rate limits should a MODE2 AHP server enforce?" as: 1,419ms / **3,165ms** / 1,131ms — a ±1,100ms standard deviation with one outlier run of 3.2 seconds against a baseline of ~1.3 seconds. The paper reports this as `1,905ms ±1,100ms` in the comparison table without explanation.

A nearly 2.5× latency spike on a single run is likely an API timeout, cold start, or transient rate limit on the Anthropic API during the benchmark run. It's not measurement error — it's a real event that happened to coincide with this query's second run. The paper should note it: "One run of the rate limits RAG query showed a 3.2s latency (vs. ~1.3s typical), likely reflecting API response variance; this inflates the stddev for that row."

Without this note, a reader might conclude that RAG latency on rate-limits queries is inherently more variable than on other queries, which is not supported by the data.

### 6. The RAG stddev of ±1.0 for the rate-limits query is suspicious in the opposite direction

Simultaneously, the *token count* stddev for the same query is ±1.0 (tokens: 563, 565, 564). While this is plausible for a query with a narrow, deterministic answer drawn from a highly specific context, it contrasts sharply with the other queries (±6 to ±31 tokens). The combination of ±1 token variance and ±1,100ms latency variance in the same query and run is anomalous — the content was stable but the infrastructure was not. This deserves a note, not because the numbers are necessarily wrong, but because the asymmetry is curious.

### 7. Naive latency is measured once and reused — presentation implies three independent measurements

The live raw_runs data shows `naive_latency_ms: 165.09304801002145` repeated verbatim across all three runs of every AHP site query, and `192.6610310038086` across all Nate site queries. The document fetch happens once per `run_token_comparison_multi` call; the latency is stored and copied into each run record.

This is correct behaviour (you fetch the full document once), but the "3-run mean ± stddev" presentation implies equivalent measurement methodology across all three baselines. AHP latency is genuinely 3 independent measurements. Naive latency is 1 measurement inserted 3 times. The table footnote says naive baseline is "not API-measured" — this is good — but it should additionally note that the latency figure is a single measurement, not a 3-run mean.

### 8. The abstract's "~20ms p50 cache-hit latency" understates the measured value

The abstract says "~20ms p50 cache-hit latency." The live profile reports p50 = 20.6ms. This rounds correctly to "~20ms" and is not a material error, but a "~20ms" in the abstract sourced from a profile that is 100% cache hits (no cold samples) slightly undersells the caution that belongs there. The full sentence should read "~20ms p50 cache-hit latency (all 20 profile samples were cache hits; cold-cache latency is 2,500–5,200ms — see §4.3)."

### 9. The appendix still says "Test Suite (v2)" — four revisions behind

The artefacts table at the end of the paper lists:
> `Test Suite (v2) | https://github.com/AHP-Organization/test-suite`

The paper refers to "v4" in the abstract, introduction, Section 3.3, Section 4.1, Section 4.2, and Section 4.3. This is the fourth time this stale version number has appeared in the appendix across four critique cycles. It will be caught by any reviewer who reads to the end of the paper.

### 10. Section 4.1 conformance table lists "T11: Note: 4 tool calls for a 2-lookup query" — the live data contradicts this

The conformance table note for T11 says: *"Note: 4 tool calls for a 2-lookup query; see §4.4."* The live T11 result shows `tools_used: ["check_inventory"]` — exactly 1 tool call, not 4. The v3 run (08:10) showed 4 tool calls; the current run shows 1. The double-invocation issue may have been fixed in the reference server between runs, or it may be non-deterministic. Either way, the footnote is now factually incorrect based on the current test data. The paper should reflect what the current run shows, not a stale observation from two runs ago.

### 11. The Conclusion is stale in two places

The Conclusion (§7) contains:
- *"We demonstrated **18/19** conformance on the reference deployment (T17 fails by design in demo mode; T18 session-limit and T19 oversized-body tests both pass in their first v3 run)"* — This is the old v3 figure. With T08 correctly failing, it should be 17/19. The parenthetical says "v3 run" which is accidentally truthful.
- *"13ms cached latency (p50)"* — The current profile shows 20.6ms p50. The "13ms" figure is from the 08:10 run. One of these is wrong; they can't both be the paper's current p50.

---

## What the Tests Need

### Must fix before claiming v4 results

1. **Run the v4 suite and use those results.** The current test data is from v3. The v4 fixes (T08 `no_memory_phrases`, latency profile redesign, T20) are in the code but have never been exercised. The paper should not claim v4 results until a v4 run has completed and the JSON is updated.

2. **Verify T08 actually fails after the fix runs.** The v4 `no_memory_phrases` list is comprehensive and should correctly catch the server's "I don't have any record" response. But this is unverified until the test actually executes. The current position (claiming T08 fails based on editorial knowledge while the backing data says it passes) is untenable.

3. **Run the latency profile with the v4 design** (10 cold nonces + 10 hot). Report `cold_mean_ms`, `cold_p50_ms`, `hot_p50_ms` from the structured output. The split-design methodology in the code is correct; it just needs to run.

4. **Report T20 results.** The spec §11.1 MUST requirement for rate-limit headers is currently unverified. T20 is in the code; it needs to execute and the result needs to appear in the paper. Given that T16 has reported "unknown/unknown" for rate-limit headers across three consecutive runs, T20 may well fail — which would be a new conformance issue.

### Remaining coverage gaps

5. **`clarification_needed` response flow is untested** across all 4 suite versions. Section 6.3 of the spec defines a full clarification protocol. Any implementation that never returns `clarification_needed` passes all 20 tests.

6. **Content type negotiation is untested.** Section 6.6 defines `accept_types`, `response_types`, `accept_fallback`, and `unsupported_type`. Zero tests exercise this.

7. **Session expiry (10-minute TTL) is untested.** The suite enforces the 10-turn limit (T18) but not the time limit. A server that never expires sessions passes.

8. **T12 tightened assertion is still imperfect.** The failure-phrase exclusion list catches common tool-failure patterns, but a sufficiently verbose error message could still pass (e.g., "I've prepared a preliminary **quote** structure for you, though I was **unable to** find exact pricing"). End-to-end validation against known fixture prices remains the reliable approach.

9. **T16 window state is still "unknown/unknown"** across all runs. If T20 verifies these headers and they're consistently absent, the spec §11.1 MUST requirement is violated. The suite should not just record "unknown" and move on — the T20 logic to assert presence and numeric values is the right approach.

---

## What the Whitepaper Needs

### Corrections required immediately

1. **Re-run with v4 and update all data tables.** This is the only way to resolve the v3/v4 mismatch. The RAG baseline numbers (289–951 AHP site; 425–650 Nate site) are from the v3 run and happen to match the paper's tables — which is fortunate, but the paper still needs a v4 run to validate T08, the latency profile, and T20 before submission.

2. **Harmonise the three conformance figures.** The abstract, Section 4.1 summary, and Conclusion must all state the same number. The correct v4 figure (once that run completes) will be: 17/20 if T08 and T17 both fail and T20 passes; 16/20 if T20 also fails. Until v4 runs, the paper should say "17/19 confirmed from v3 run (T08 ✗ editorial correction, T17 ✗; T20 and latency profile pending v4 run)."

3. **Fix the stale T11 footnote.** The "4 tool calls" note in the conformance table is contradicted by the current live data (1 tool call). Either update to reflect the current observation or remove the note.

4. **Fix the Conclusion p50.** "13ms cached latency (p50)" conflicts with both the current profile (20.6ms) and the abstract ("~20ms"). Pick one consistent figure.

5. **Update the appendix** from "Test Suite (v2)" to "Test Suite (v4)."

### Framing improvements

6. **Be explicit in the abstract that the latency profile is 100% cache hits.** The abstract says "~20ms p50 cache-hit latency" — the "(cache-hit)" qualifier is good, but a reader should immediately understand this is not the cold-cache experience. Add: "(cold-cache: 2.5–5.2s; see §4.3)."

7. **The version claim in Section 4.2 should be honest about the mismatch.** Replace *"Results from 2026-02-22 09:00 UTC run — first complete run with cache-busted RAG baseline API-measured (v4 suite)"* with something like: *"Results from 09:04 UTC run (v3 suite with cache-busting and RAG baseline; T08 result manually corrected — v4 fix not yet run at time of publication). T20 and latency profile pending v4 run."*

8. **The §3.1 session memory disclosure needs to carry more weight.** The limitation is disclosed — that's good. But the spec's §6.2 session memory requirement is described as a "spec requirement" without emphasis on the fact that the reference implementation ships without it. A practitioner building on the reference implementation will encounter this gap at integration time. The disclosure should include a practical warning: *"Developers building on the v0 reference implementation must implement their own conversation history injection before deploying a spec-compliant MODE2 server."*

9. **Explain the RAG latency outlier in §4.2 (rate-limits query, 3.2s run 2).** One sentence in a table footnote is sufficient: "†Run 2 of the rate-limits RAG query recorded 3.2s latency (vs. ~1.3s typical), likely reflecting API variance; this inflates the stddev for that row."

---

## Summary of Most Critical Issues

- **The whitepaper claims v4 suite results but the live data is definitively from v3** (T20 absent, T08 falsely passing, latency profile all-cached with no v4 fields). The v4 fixes are in the code but have never run. The paper must either execute a genuine v4 run and update tables, or explicitly label the data as v3 with manual corrections.

- **Three inconsistent conformance numbers appear in the same paper**: the abstract says 16/19, Section 4.1 says 16/20, the Conclusion says 18/19. All three are wrong. With T08 and T17 both failing, the correct figure is 17/19 (or 17/20 once T20 is added). A reviewer comparing abstract to conclusion will flag this immediately.

- **T08 is still a live false pass** — the server says "I don't have any record of previous questions you've asked. This is the first message in our conversation" and the test reports "Session memory verified." The v4 fix is in the code but unexercised. The conformance table correctly marks T08 as ✗ based on editorial knowledge, but the backing data contradicts it.

- **The latency profile is still entirely cache hits** (20/20), and the cold-cache profile promised by the v4 redesign doesn't exist yet in the data. The paper's latency section presents a comparison table with "Cold-cache latency inferred from individual tests" as a workaround, which is the honest framing — but the 20-sample profile itself only measures cache performance, not MODE2 operation under normal conditions.

- **T20 (rate-limit headers) has never run** but the spec §11.1 MUST requirement it tests has been observed as "unknown/unknown" in three consecutive T16 runs. This is either a server conformance failure or a client-side header-reading bug — and it is currently unresolved. This test should be the first to execute in the v4 run.
